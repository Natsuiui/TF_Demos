{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4544d460",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing import sequence\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "import os\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2f7a73ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_file = tf.keras.utils.get_file('shakespeare.txt', 'https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d0ce396d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of text: 1115394 characters\n"
     ]
    }
   ],
   "source": [
    "# Read, then decode for py2 compat.\n",
    "text = open(path_to_file, 'rb').read().decode(encoding='utf-8')\n",
    "# length of text is the number of characters in it\n",
    "print ('Length of text: {} characters'.format(len(text)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c934023b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You are all resolved rather to die than to famish?\n",
      "\n",
      "All:\n",
      "Resolved. resolved.\n",
      "\n",
      "First Citizen:\n",
      "First, you know Caius Marcius is chief enemy to the people.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Take a look at the first 250 characters in text\n",
    "print(text[:250])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "508fd5b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = sorted(set(text))\n",
    "# Creating a mapping from unique characters to indices\n",
    "char2idx = {u:i for i, u in enumerate(vocab)}\n",
    "idx2char = np.array(vocab)\n",
    "\n",
    "def text_to_int(text):\n",
    "    return np.array([char2idx[c] for c in text])\n",
    "\n",
    "text_as_int = text_to_int(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "13a7b30f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: First Citizen\n",
      "Encoded: [18 47 56 57 58  1 15 47 58 47 64 43 52]\n"
     ]
    }
   ],
   "source": [
    "# lets look at how part of our text is encoded\n",
    "print(\"Text:\", text[:13])\n",
    "print(\"Encoded:\", text_to_int(text[:13]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1c74b6d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Citizen\n"
     ]
    }
   ],
   "source": [
    "def int_to_text(ints):\n",
    "    try:\n",
    "        ints = ints.numpy()\n",
    "    except:\n",
    "        pass\n",
    "    return ''.join(idx2char[ints])\n",
    "\n",
    "print(int_to_text(text_as_int[:13]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a03d7571",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "seq_length = 100  # length of sequence for a training example\n",
    "examples_per_epoch = len(text)//(seq_length+1)\n",
    "\n",
    "# Create training examples / targets\n",
    "char_dataset = tf.data.Dataset.from_tensor_slices(text_as_int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "509526e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences = char_dataset.batch(seq_length+1, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bde992f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_input_target(chunk):  # for the example: hello\n",
    "    input_text = chunk[:-1]  # hell\n",
    "    target_text = chunk[1:]  # ello\n",
    "    return input_text, target_text  # hell, ello\n",
    "\n",
    "dataset = sequences.map(split_input_target)  # we use map to apply the above function to every entry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ce50471c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "EXAMPLE\n",
      "\n",
      "INPUT\n",
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You\n",
      "\n",
      "OUTPUT\n",
      "irst Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You \n",
      "\n",
      "\n",
      "EXAMPLE\n",
      "\n",
      "INPUT\n",
      "are all resolved rather to die than to famish?\n",
      "\n",
      "All:\n",
      "Resolved. resolved.\n",
      "\n",
      "First Citizen:\n",
      "First, you \n",
      "\n",
      "OUTPUT\n",
      "re all resolved rather to die than to famish?\n",
      "\n",
      "All:\n",
      "Resolved. resolved.\n",
      "\n",
      "First Citizen:\n",
      "First, you k\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-13 23:20:01.854491: W tensorflow/tsl/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz\n"
     ]
    }
   ],
   "source": [
    "for x, y in dataset.take(2):\n",
    "    print(\"\\n\\nEXAMPLE\\n\")\n",
    "    print(\"INPUT\")\n",
    "    print(int_to_text(x))\n",
    "    print(\"\\nOUTPUT\")\n",
    "    print(int_to_text(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "78f80236",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "VOCAB_SIZE = len(vocab)  # vocab is number of unique characters\n",
    "EMBEDDING_DIM = 256\n",
    "RNN_UNITS = 1024\n",
    "\n",
    "# Buffer size to shuffle the dataset\n",
    "# (TF data is designed to work with possibly infinite sequences,\n",
    "# so it doesn't attempt to shuffle the entire sequence in memory. Instead,\n",
    "# it maintains a buffer in which it shuffles elements).\n",
    "BUFFER_SIZE = 10000\n",
    "\n",
    "data = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9debe63b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (64, None, 256)           16640     \n",
      "                                                                 \n",
      " lstm (LSTM)                 (64, None, 1024)          5246976   \n",
      "                                                                 \n",
      " dense (Dense)               (64, None, 65)            66625     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 5,330,241\n",
      "Trainable params: 5,330,241\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "def build_model(vocab_size, embedding_dim, rnn_units, batch_size):\n",
    "    model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(vocab_size, embedding_dim,\n",
    "                              batch_input_shape=[batch_size, None]),\n",
    "    tf.keras.layers.LSTM(rnn_units,\n",
    "                        return_sequences=True,\n",
    "                        stateful=True,\n",
    "                        recurrent_initializer='glorot_uniform'),\n",
    "    tf.keras.layers.Dense(vocab_size)\n",
    "  ])\n",
    "    return model\n",
    "\n",
    "model = build_model(VOCAB_SIZE,EMBEDDING_DIM, RNN_UNITS, BATCH_SIZE)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a878eac5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 100, 65) # (batch_size, sequence_length, vocab_size)\n"
     ]
    }
   ],
   "source": [
    "for input_example_batch, target_example_batch in data.take(1):\n",
    "    example_batch_predictions = model(input_example_batch)  # ask our model for a prediction on our first batch of training data (64 entries)\n",
    "    print(example_batch_predictions.shape, \"# (batch_size, sequence_length, vocab_size)\")  # print out the output shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5e3fc506",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64\n",
      "tf.Tensor(\n",
      "[[[ 1.7852017e-03  6.1457808e-04 -2.4946202e-03 ...  5.4947417e-03\n",
      "    5.0941012e-03 -8.8060347e-05]\n",
      "  [ 1.1672266e-05 -1.4731460e-03  1.0857894e-03 ... -9.9544693e-04\n",
      "    8.1720622e-03  1.7027390e-03]\n",
      "  [-3.3933839e-03 -1.9652062e-04 -1.2926784e-03 ...  3.5510403e-03\n",
      "    4.9258387e-03 -1.5528948e-03]\n",
      "  ...\n",
      "  [-1.0975113e-03  2.8378682e-03 -8.9973118e-03 ...  1.1726836e-03\n",
      "    3.8203404e-03  1.0832231e-03]\n",
      "  [ 5.8180867e-03 -5.0577195e-03 -7.8686960e-03 ...  1.1396753e-03\n",
      "    4.5869416e-03 -5.7543744e-04]\n",
      "  [ 1.1292380e-02 -2.7070572e-03 -8.7875417e-03 ...  6.9863014e-03\n",
      "    4.0852749e-03 -2.2873515e-03]]\n",
      "\n",
      " [[ 1.7852017e-03  6.1457808e-04 -2.4946202e-03 ...  5.4947417e-03\n",
      "    5.0941012e-03 -8.8060347e-05]\n",
      "  [ 7.0557441e-03  3.5182727e-03 -2.7715107e-03 ...  9.4597945e-03\n",
      "    2.7606145e-03 -2.6310226e-03]\n",
      "  [ 1.6668388e-03  9.7116167e-03  1.0372318e-03 ...  5.7866001e-03\n",
      "    3.0862524e-03 -5.8719906e-04]\n",
      "  ...\n",
      "  [ 1.1847350e-02 -2.0987585e-03 -2.2501824e-03 ...  5.4496585e-04\n",
      "    3.1665827e-03  4.7773342e-03]\n",
      "  [ 1.2074580e-02 -3.0474504e-04  9.4167760e-04 ...  2.9491528e-03\n",
      "    3.5304793e-03  1.2246347e-02]\n",
      "  [ 1.0002967e-02  3.5035482e-04 -3.2211309e-03 ...  6.9266250e-03\n",
      "    8.5602282e-03  1.1141107e-02]]\n",
      "\n",
      " [[ 2.6687694e-04 -2.1478912e-04 -1.7068146e-03 ...  8.7080067e-03\n",
      "    1.7729900e-03  2.4523330e-03]\n",
      "  [ 2.9693753e-03 -1.5587450e-03 -1.7605062e-03 ...  5.1556714e-03\n",
      "    4.2211572e-03  4.0910440e-03]\n",
      "  [ 8.3225733e-03 -1.9664471e-03 -5.9084757e-03 ...  4.0717460e-03\n",
      "    4.8758658e-03 -1.9226258e-03]\n",
      "  ...\n",
      "  [-4.7940672e-03  8.2450062e-03 -9.0301898e-04 ...  6.6612649e-04\n",
      "   -3.9007945e-04  6.5516038e-03]\n",
      "  [ 1.6521510e-03  7.9763122e-03  3.1232664e-03 ...  4.8673111e-03\n",
      "   -5.0057354e-04  1.0599254e-02]\n",
      "  [ 8.2164565e-03  9.1848103e-03 -1.5265762e-04 ...  9.3871849e-03\n",
      "   -7.0518820e-04  6.5647420e-03]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[-2.4046309e-03 -2.6361977e-03 -6.4884662e-05 ... -2.6681121e-03\n",
      "    2.2856756e-03 -1.8617848e-03]\n",
      "  [ 4.8606824e-03  6.6565792e-04 -1.0297827e-03 ...  2.7338364e-03\n",
      "    1.7082874e-03 -2.6253192e-03]\n",
      "  [ 7.6180478e-03  1.6670909e-03  3.1085319e-03 ...  4.9488656e-03\n",
      "    2.3444151e-03  5.8095860e-03]\n",
      "  ...\n",
      "  [ 1.1363789e-03  7.9678213e-03  2.3107766e-03 ...  7.4234572e-03\n",
      "   -4.4112625e-03  3.2373676e-03]\n",
      "  [ 3.4329589e-03  2.1671732e-03  3.8070972e-03 ...  4.7713704e-03\n",
      "    6.3585374e-04  3.8912143e-03]\n",
      "  [ 1.1556937e-03  5.4311368e-04  2.7766393e-03 ...  6.9098286e-03\n",
      "   -1.3677372e-03  3.6224858e-03]]\n",
      "\n",
      " [[ 1.0386527e-03  7.1195100e-04 -4.8051234e-03 ... -4.7110133e-03\n",
      "   -6.4331410e-04  3.0709454e-03]\n",
      "  [-2.7303388e-03  7.5930343e-03  9.9701807e-04 ... -5.0777998e-03\n",
      "    8.9311734e-04  3.5292120e-03]\n",
      "  [-5.6299232e-03  2.1589957e-03  2.4716547e-03 ... -7.8895316e-04\n",
      "   -4.3019224e-03  2.0270364e-03]\n",
      "  ...\n",
      "  [-3.3469694e-03 -2.3292475e-03  7.5372290e-03 ...  1.0554684e-02\n",
      "    5.3873532e-03  1.7858125e-02]\n",
      "  [-3.4121163e-03 -5.6613381e-03  1.0017930e-02 ...  7.9302816e-03\n",
      "    1.3204159e-03  9.5941406e-03]\n",
      "  [-5.3105759e-03  1.0141876e-03  1.0828286e-02 ...  4.1319435e-03\n",
      "    2.3878079e-03  8.2909539e-03]]\n",
      "\n",
      " [[ 4.2289286e-03 -4.8902817e-03 -7.1951083e-04 ... -1.4149239e-03\n",
      "    8.1643334e-04 -9.9658547e-04]\n",
      "  [-3.1806238e-05  3.0097456e-03  1.5808325e-03 ... -2.4262443e-03\n",
      "    1.4466359e-03  6.0412123e-05]\n",
      "  [-1.4383241e-03  8.9892535e-04  2.4530545e-03 ... -7.3192865e-03\n",
      "    5.6239311e-03  2.1854171e-03]\n",
      "  ...\n",
      "  [ 5.5700969e-03  2.4354234e-03  5.5901147e-04 ...  1.4940406e-02\n",
      "    3.5300194e-03  6.9043711e-03]\n",
      "  [ 2.2803883e-03  8.1715994e-03  3.0917632e-03 ...  1.0838144e-02\n",
      "    3.2771993e-03  5.5471705e-03]\n",
      "  [-8.6514149e-03  4.9787238e-03 -2.5276658e-03 ...  4.7064857e-03\n",
      "    2.5476504e-03  5.4974770e-03]]], shape=(64, 100, 65), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# we can see that the predicition is an array of 64 arrays, one for each entry in the batch\n",
    "print(len(example_batch_predictions))\n",
    "print(example_batch_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "03412e42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n",
      "tf.Tensor(\n",
      "[[ 1.7852017e-03  6.1457808e-04 -2.4946202e-03 ...  5.4947417e-03\n",
      "   5.0941012e-03 -8.8060347e-05]\n",
      " [ 1.1672266e-05 -1.4731460e-03  1.0857894e-03 ... -9.9544693e-04\n",
      "   8.1720622e-03  1.7027390e-03]\n",
      " [-3.3933839e-03 -1.9652062e-04 -1.2926784e-03 ...  3.5510403e-03\n",
      "   4.9258387e-03 -1.5528948e-03]\n",
      " ...\n",
      " [-1.0975113e-03  2.8378682e-03 -8.9973118e-03 ...  1.1726836e-03\n",
      "   3.8203404e-03  1.0832231e-03]\n",
      " [ 5.8180867e-03 -5.0577195e-03 -7.8686960e-03 ...  1.1396753e-03\n",
      "   4.5869416e-03 -5.7543744e-04]\n",
      " [ 1.1292380e-02 -2.7070572e-03 -8.7875417e-03 ...  6.9863014e-03\n",
      "   4.0852749e-03 -2.2873515e-03]], shape=(100, 65), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# lets examine one prediction\n",
    "pred = example_batch_predictions[0]\n",
    "print(len(pred))\n",
    "print(pred)\n",
    "# notice this is a 2d array of length 100, \n",
    "#where each interior array is the prediction for the next character at each time step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5d01032c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "65\n",
      "tf.Tensor(\n",
      "[ 1.7852017e-03  6.1457808e-04 -2.4946202e-03  1.8026571e-03\n",
      "  2.8827037e-03 -2.2151083e-04 -3.6322160e-04  2.0673471e-03\n",
      "  9.2511596e-03  4.5862771e-04  4.7195391e-03  8.7708505e-03\n",
      " -8.5869133e-03 -1.6229483e-03 -5.3940774e-03  5.3539989e-05\n",
      "  3.5815658e-03  2.8615296e-03 -4.5555220e-03 -1.6005746e-03\n",
      "  4.1341810e-03  1.5032027e-03  2.1295692e-04  1.5932377e-03\n",
      "  2.7101119e-03 -1.6926206e-03  1.1901625e-03 -1.1284995e-03\n",
      "  4.2640842e-03  4.4058114e-03  2.5339003e-03  3.5841279e-03\n",
      "  5.3705066e-05 -6.5710291e-04  1.8018604e-03 -5.3375983e-03\n",
      " -9.3271001e-04 -6.6649378e-04  1.0875711e-03  2.2319076e-04\n",
      " -6.6640868e-04  3.0010531e-03 -4.7944738e-03 -2.0600269e-03\n",
      " -3.9278162e-03 -3.0950962e-03 -2.4123751e-03 -1.3126438e-03\n",
      " -4.3254482e-04  4.3852152e-03 -7.6125921e-03  3.1936341e-03\n",
      " -6.2210076e-03 -2.0423601e-03 -8.4540644e-04  2.3380013e-03\n",
      "  9.4754878e-04 -7.3730975e-04  7.3731486e-03 -3.0999135e-03\n",
      " -1.4885095e-03  2.6145708e-03  5.4947417e-03  5.0941012e-03\n",
      " -8.8060347e-05], shape=(65,), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# and finally well look at a prediction at the first timestep\n",
    "time_pred = pred[0]\n",
    "print(len(time_pred))\n",
    "print(time_pred)\n",
    "# and of course its 65 values representing the probabillity of each character occuring next"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d15a7d32",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\".P-b;MrUF'!qa!LErTRQ&yV&yls:Yz,M\\nIMftcF3s!Wph,XmXX J&kUoz;:Z$dn3fjG-IfcYHlwUI.x Z,MQDmr$-CbdKUV-?QwW\""
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# If we want to determine the predicted character we need to sample the output distribution (pick a value based on probabillity)\n",
    "sampled_indices = tf.random.categorical(pred, num_samples=1)\n",
    "\n",
    "# now we can reshape that array and convert all the integers to numbers to see the actual characters\n",
    "sampled_indices = np.reshape(sampled_indices, (1, -1))[0]\n",
    "predicted_chars = int_to_text(sampled_indices)\n",
    "\n",
    "predicted_chars  # and this is what the model predicted for training sequence 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "82151727",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(labels, logits):\n",
    "    return tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1223e6e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss=loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5111be39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directory where the checkpoints will be saved\n",
    "checkpoint_dir = './training_checkpoints'\n",
    "# Name of the checkpoint files\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
    "\n",
    "checkpoint_callback=tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_prefix,\n",
    "    save_weights_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4eac358",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "172/172 [==============================] - 192s 1s/step - loss: 2.5999\n",
      "Epoch 2/50\n",
      "172/172 [==============================] - 220s 1s/step - loss: 1.8894\n",
      "Epoch 3/50\n",
      "172/172 [==============================] - 214s 1s/step - loss: 1.6387\n",
      "Epoch 4/50\n",
      "172/172 [==============================] - 253s 1s/step - loss: 1.5057\n",
      "Epoch 5/50\n",
      "172/172 [==============================] - 268s 2s/step - loss: 1.4226\n",
      "Epoch 6/50\n",
      "172/172 [==============================] - 280s 2s/step - loss: 1.3655\n",
      "Epoch 7/50\n",
      "172/172 [==============================] - 282s 2s/step - loss: 1.3208\n",
      "Epoch 8/50\n",
      "  4/172 [..............................] - ETA: 4:01 - loss: 1.2732"
     ]
    }
   ],
   "source": [
    "history = model.fit(data, epochs=50, callbacks=[checkpoint_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b3513d29",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = build_model(VOCAB_SIZE, EMBEDDING_DIM, RNN_UNITS, batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c7963ab1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights(tf.train.latest_checkpoint(checkpoint_dir))\n",
    "model.build(tf.TensorShape([1, None]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "73e620da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(model, start_string):\n",
    "  # Evaluation step (generating text using the learned model)\n",
    "\n",
    "  # Number of characters to generate\n",
    "    num_generate = 800\n",
    "\n",
    "  # Converting our start string to numbers (vectorizing)\n",
    "    input_eval = [char2idx[s] for s in start_string]\n",
    "    input_eval = tf.expand_dims(input_eval, 0)\n",
    "\n",
    "  # Empty string to store our results\n",
    "    text_generated = []\n",
    "\n",
    "  # Low temperatures results in more predictable text.\n",
    "  # Higher temperatures results in more surprising text.\n",
    "  # Experiment to find the best setting.\n",
    "    temperature = 1.0\n",
    "\n",
    "  # Here batch size == 1\n",
    "    model.reset_states()\n",
    "    for i in range(num_generate):\n",
    "        predictions = model(input_eval)\n",
    "      # remove the batch dimension\n",
    "    \n",
    "        predictions = tf.squeeze(predictions, 0)\n",
    "\n",
    "      # using a categorical distribution to predict the character returned by the model\n",
    "        predictions = predictions / temperature\n",
    "        predicted_id = tf.random.categorical(predictions, num_samples=1)[-1,0].numpy()\n",
    "\n",
    "      # We pass the predicted character as the next input to the model\n",
    "      # along with the previous hidden state\n",
    "        input_eval = tf.expand_dims([predicted_id], 0)\n",
    "\n",
    "        text_generated.append(idx2char[predicted_id])\n",
    "\n",
    "    return (start_string + ''.join(text_generated))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "905f1795",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter the starting string Hello\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'generate_text' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m inp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28minput\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEnter the starting string \u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mgenerate_text\u001b[49m(model, inp))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'generate_text' is not defined"
     ]
    }
   ],
   "source": [
    "inp = input(\"Enter the starting string \")\n",
    "print(generate_text(model, inp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5e43850",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
